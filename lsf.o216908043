Sender: LSF System <lsfadmin@eu-g3-072>
Subject: Job 216908043: <synthdata> in cluster <euler> Exited

Job <synthdata> was submitted from host <eu-login-40> by user <jmattern> in cluster <euler> at Mon May  2 23:16:10 2022
Job was executed on host(s) <20*eu-g3-072>, in queue <gpuhe.24h>, as user <jmattern> in cluster <euler> at Mon May  2 23:17:01 2022
</cluster/home/jmattern> was used as the home directory.
</cluster/work/sachan/jmattern/private-datasets> was used as the working directory.
Started at Mon May  2 23:17:01 2022
Terminated at Mon May  2 23:17:56 2022
Results reported at Mon May  2 23:17:56 2022

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python train_lm.py --model gpt2-xl --tokenizer gpt2-xl --epochs 5 --batch-size 1
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   3.28 sec.
    Max Memory :                                 281 MB
    Average Memory :                             84.25 MB
    Total Requested Memory :                     20480.00 MB
    Delta Memory :                               20199.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   79 sec.
    Turnaround time :                            106 sec.

The output (if any) follows:

Traceback (most recent call last):
  File "train_lm.py", line 5, in <module>
    from private_transformers import PrivacyEngine
ModuleNotFoundError: No module named 'private_transformers'
